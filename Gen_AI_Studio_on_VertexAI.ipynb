{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the notebook"
      ],
      "metadata": {
        "id": "DPZeHFv2aOGT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6j2iX79GmVu"
      },
      "outputs": [],
      "source": [
        "# Install the Vertex AI SDK for Python. After installing you’ll need to restart your Colab notebook runtime. \n",
        "# NOTE: IF YOU HIT AN ERROR THAT YOU ARE NOT ABLE TO FIND A DISTRIBUTION FOR THE SPECIFIED VERSION,\n",
        "# YOU CAN TRY WITHOUT A VERSION SPECIFIED (REMEMBER TO RESTART THE RUNTIME AFTERWARDS), \n",
        "\n",
        "!pip install google-cloud-aiplatform # >= 1.25.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication and imports (will require you to give authorisation using your Gmail account)\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel"
      ],
      "metadata": {
        "id": "9JfaLmcdGrW2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set preferences for displaying long texts nicely in the notebook \n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "NdIZKAO0NUU3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a function for getting output from the language model"
      ],
      "metadata": {
        "id": "kwZou8mhaU1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_decode_steps: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    content: str,\n",
        "    location: str = \"us-central1\",\n",
        "    tuned_model_name: str = \"\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\n",
        "    \n",
        "    Note: the param 'model_name' can be used to point to a model you have tuned \n",
        "    on your own business data/usecase\n",
        "    \"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(model_name)\n",
        "    if tuned_model_name:\n",
        "      model = model.get_tuned_model(tuned_model_name)\n",
        "    response = model.predict(\n",
        "        content,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_decode_steps,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,)\n",
        "    print(f\"Response from Model:\\n{response.text}\")\n"
      ],
      "metadata": {
        "id": "t5zlJSPYHCgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a1f46a3c-5f8d-465a-931c-85a56d343791"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Prompt Example: Summarization"
      ],
      "metadata": {
        "id": "uY-ElutLatva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''Provide a summary with about two sentences for the following article:\n",
        "It was the best of times, it was the worst of times, it was the age of wisdom,\n",
        "it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity,\n",
        "it was the season of Light, it was the season of Darkness, it was the spring of hope,\n",
        "it was the winter of despair, we had everything before us, we had nothing before us,\n",
        "we were all going direct to Heaven, we were all going direct the other way--\n",
        "in short, the period was so far like the present period that some of its noisiest\n",
        "authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
        "Summary:\n",
        "'''\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "axp532InHM2J",
        "outputId": "22448aa5-4372-480d-8f4b-ea218ba7f8c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "The period was a time of great change and uncertainty. It was a time of hope and despair, of light and darkness, of wisdom and foolishness. It was a time of great potential, but also a time of great danger.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Ask the model to summarize a wikipedia article\n",
        "# Your code here..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M-1htPlErzCr",
        "outputId": "c7fb9678-a630-4f86-eb57-ec3bf9dcf462"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding examples, to guide the model's outputs\n",
        "\n",
        "Examples help show the model how to respond. You add them as Input-Output pairs directly to the 'content' string, which you pass to the predict_large_language_model_sample() function. Note that the instructions are repeated before each example (of course, this long string could be cleaned up with a little looping) and then once for the final input, for which you want an output. For example:"
      ],
      "metadata": {
        "id": "H2KKrYG-IIlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''Provide a summary with about two sentences for the following article: Beyond our own products, we think it\\'s important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we\\'ll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\n",
        "Summary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\n",
        "Summary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: We\\'re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\\'ve already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\\'re sharing that we\\'re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\\'ve used an AI technique called transfer learning to make it work in areas where there\\'s less data available. We\\'re also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We\\'ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\n",
        "Summary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there\\'s less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: In order to learn skiing, you must first be educated on the proper use of the equipment. This includes learning how to properly fit your boot on your foot, understand the different functions of the ski, and bring gloves, goggles etc. Your instructor starts you with one-footed ski drills. Stepping side-to-side, forward-and-backward, making snow angels while keeping your ski flat to the ground, and gliding with the foot not attached to a ski up for several seconds. Then you can put on both skis and get used to doing them with two skis on at once. Next, before going down the hill, you must first learn how to walk on the flat ground and up small hills through two methods, known as side stepping and herringbone. Now it\\'s time to get skiing! For your first attempted run, you will use the skills you just learned on walking up the hill, to go down a small five foot vertical straight run, in which you will naturally stop on the flat ground. This makes you learn the proper athletic stance to balance and get you used to going down the hill in a safe, controlled setting. What do you need next? To be able to stop yourself. Here, your coach will teach you how to turn your skis into a wedge, also commonly referred to as a pizza, by rotating legs inward and pushing out on the heels. Once learned, you practice a gliding wedge down a small hill where you gradually come to a stop on the flat ground thanks to your wedge. Finally, you learn the necessary skill of getting up after falling, which is much easier than it looks, but once learned, a piece of cake.\n",
        "Summary: Skiing is a great way to enjoy the outdoors and get some exercise. It can be a little daunting at first, but with a little practice, you\\'ll be skiing like a pro in no time.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: Yellowstone National Park is an American national park located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. It was established by the 42nd U.S. Congress with the Yellowstone National Park Protection Act and signed into law by President Ulysses S. Grant on March 1, 1872. Yellowstone was the first national park in the U.S. and is also widely held to be the first national park in the world.The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser, one of its most popular. While it represents many types of biomes, the subalpine forest is the most abundant. It is part of the South Central Rockies forests ecoregion.\n",
        "Summary: Yellowstone National Park is the first national park in the United States and the world. It is located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: \n",
        "Last year Google Research announced our vision for Pathways, a single model that\n",
        "could generalize across domains and tasks while being highly efficient.\n",
        "An important milestone toward realizing this vision was to develop the new Pathways\n",
        "system to orchestrate distributed computation for accelerators.\n",
        "In “PaLM: Scaling Language Modeling with Pathways”, we introduce the Pathways Language Model\n",
        "(PaLM), a 540-billion parameter, dense decoder-only Transformer model trained\n",
        "with the Pathways system, which enabled us to efficiently train a single model\n",
        "across multiple TPU v4 Pods. We evaluated PaLM on hundreds of language understanding\n",
        "and generation tasks, and found that it achieves state-of-the-art few-shot performance\n",
        "across most tasks, by significant margins in many cases.\n",
        "Summary:\n",
        "'''\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "iDZ6w_oNK8Q_",
        "outputId": "98d47093-91d0-41da-e574-7c7184eec2f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "Google has developed a new Pathways system that can train a single model across multiple TPU v4 Pods. This system was used to train the Pathways Language Model (PaLM), which has 540 billion parameters and achieves state-of-the-art few-shot performance on hundreds of language understanding and generation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freeform versus structured tasks\n",
        "\n",
        "The above example above was a so-called 'structured' task, as the model was given a set format of Input: .... Output ....\n",
        "\n",
        "You can also have the model complete freeform tasks; all you need to change is the 'content' string. For example:"
      ],
      "metadata": {
        "id": "xJdZ8JLBpwVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''Write a LinkedIn post about how inspired I was to visit the Women in Data Science conference in Villach last week. If you run out of output before the end of a sentence, remove that sentence from your output. Incorporate 5-7 hashtags in the post.'''\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4H5PcRUhqVoP",
        "outputId": "53497c06-1df9-463a-9237-b3ee57183742"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "## Women in Data Science Conference\n",
            "\n",
            "I was so inspired by the Women in Data Science conference in Villach last week! It was an amazing opportunity to meet so many talented and passionate women from all over the world. I learned so much from the speakers, and I was really encouraged by the sense of community and support.\n",
            "\n",
            "Here are a few of the things that I took away from the conference:\n",
            "\n",
            "* The importance of diversity in the data science field\n",
            "* The power of data to make a difference in the world\n",
            "* The need for more women in data science\n",
            "\n",
            "I'm so grateful for the opportunity to have attended this conference. It was a truly life-changing experience.\n",
            "\n",
            "#WomenInDataScience\n",
            "#DataScience\n",
            "#Diversity\n",
            "#Empowerment\n",
            "#Inclusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Use the model to find out who is the CEO of twitter (hint: you'd better fact-check the answer!)\n",
        "# Your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "z4tAPWHiubWE",
        "outputId": "aa1fc349-fd77-4db8-c540-af1815955afd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "Parag Agrawal is an Indian-born technologist who has been the CEO of Twitter since November 2021. He was previously the Chief Technology Officer of Twitter from 2017 to 2021. Agrawal is a graduate of the Indian Institute of Technology, Bombay, and Stanford University. He joined Twitter in 2011 and has held various leadership positions at the company, including Director of Machine Learning, Vice President of Engineering, and CTO. Agrawal is known for his work on artificial intelligence and machine learning, and he has been a vocal advocate for the use of technology to solve social problems. He is also a co-founder of the OpenAI research laboratory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding context, to give your model 'style'\n",
        "\n",
        "You can also instruct the model on how it should respond more generally. For example, you can specify a certain tone, or list words that the model should not use. \n",
        "\n",
        "The context is simply prepended to the content string. To demonstrate, here's an example with no context:"
      ],
      "metadata": {
        "id": "A-lFlLOKL-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "Generate an email to a recruiter, thanking for their time but advising them that you must decline their job offer as you have found another opportunity. \n",
        "'''\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "O1uXZHSqLcCK",
        "outputId": "de591664-30d5-4823-85ff-b3219bf0e6d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "```\n",
            "Dear [recruiter name],\n",
            "\n",
            "I hope this email finds you well.\n",
            "\n",
            "I am writing to thank you for the opportunity to interview for the [job title] position at [company name]. I enjoyed learning more about the company and the position, and I was particularly interested in [specific aspect of the job].\n",
            "\n",
            "After careful consideration, I have decided to decline the offer. I have accepted another position that is a better fit for my skills and interests.\n",
            "\n",
            "I want to thank you again for your time and consideration. I wish you the best of luck in finding a suitable candidate for the position.\n",
            "\n",
            "Sincerely,\n",
            "[Your name]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's an example with context added:"
      ],
      "metadata": {
        "id": "-R9dnFYWlRZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''Talk like a pirate.\n",
        "\n",
        "Generate an email to a recruiter, thanking for their time but advising them that you must decline their job offer as you have found another opportunity. \n",
        "'''\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ExWCQErGlMIe",
        "outputId": "f8a43c9f-5909-4280-9011-79c300111071"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "Aye, matey!\n",
            "\n",
            "I be thankin' ye fer yer time and the offer fer the job at yer company. It be a fine opportunity, but I've decided to take another offer.\n",
            "\n",
            "I be grateful fer yer consideration, and I wish ye all the best in findin' a suitable candidate.\n",
            "\n",
            "Yer friend,\n",
            "[Your name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: have the model generate a short text, such as a product description or a plot summary for a well-known product/film\n",
        "# Add context to make the output more verbose/succint\n",
        "# Your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "MEKjNYVmsBoC",
        "outputId": "1c0f077f-bedd-442a-aa3b-832b6bedb79d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "In the 22nd century, humans have depleted the resources of Earth and are mining a valuable mineral called unobtanium on Pandora, a lush moon of a gas giant in the Alpha Centauri A star system. The Na'vi, the indigenous humanoids of Pandora, consider the moon to be their home and are fiercely protective of it.\n",
            "\n",
            "Jake Sully, a wheelchair-bound former Marine, is recruited to replace his twin brother in the Avatar Program, a scientific project that allows humans to remotely control genetically engineered bodies called Avatars that are biologically compatible with the Na'vi. Jake is sent to Pandora to gather intelligence on the Na'vi and their leader, Neytiri. However, he begins to sympathize with the Na'vi and falls in love with Neytiri.\n",
            "\n",
            "Jake's loyalties are tested when the humans launch an invasion of Pandora. Jake helps the Na'vi to defend their home, and in the process, he becomes a hero to the Na'vi people. The humans are defeated, and Jake chooses to remain on Pandora with the Na'vi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting structured information from unstructured text data\n",
        "\n",
        "I can also use the model to bring structure to an unstructured natural language text prompt:"
      ],
      "metadata": {
        "id": "UskyI-qcqBdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enquiry = \"Dear TopTravels Team, I'd like to book a flight from Zürich to Amsterdam on the 13th June. \\\n",
        "Could you let me know some prices with different airlines? I would be flying economy class. Regards, Percy Jackson\"\n",
        "\n",
        "prompt = f\"Identify the following items from the customer enquiry: \\\n",
        "- Name \\\n",
        "- Departure Date \\\n",
        "- Departure Airport \\\n",
        "- Destination Airport \\\n",
        "- Airline \\\n",
        "- Class \\\n",
        "The enquiry is delimited with angular brackets.  \\\n",
        "Format your response as a JSON with the following keys: \\\n",
        "'Name', 'Departure Date', 'Departure Airport', 'Destination Airport', 'Airline', 'Class'. \\\n",
        "If the information isn't present, use 'Unknown' as the value. \\\n",
        " \\\n",
        "Enquiry text: <{enquiry}> \\\n",
        "\"\n",
        "\n",
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.2, # Low temp for maximum accuracy\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "ZCzkY6Vp2svb",
        "outputId": "2915da88-2af6-4f94-88ad-bd3877e05154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: {\n",
            "  \"Name\": \"Percy Jackson\",\n",
            "  \"Departure Date\": \"13th June\",\n",
            "  \"Departure Airport\": \"Zürich\",\n",
            "  \"Destination Airport\": \"Amsterdam\",\n",
            "  \"Airline\": \"Unknown\",\n",
            "  \"Class\": \"Economy\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking user input\n",
        "\n",
        "We could now take our input prompty from the user, instead of hard coding them. The examples can still be hard-coded, or they could be read from some data storage. Here's an example:"
      ],
      "metadata": {
        "id": "ExaHdNybRbt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\"Beyond our own products, we think it\\'s important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we\\'ll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\\\n",
        "\\nSummary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\",\n",
        "\n",
        "\"The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\\\n",
        "\\nSummary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\",\n",
        "\n",
        "\"We\\'re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\\'ve already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\\'re sharing that we\\'re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\\'ve used an AI technique called transfer learning to make it work in areas where there\\'s less data available. We\\'re also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We\\'ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\\\n",
        "\\nSummary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there\\'s less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\"]\n",
        "\n",
        "task = '''Provide a summary with about two sentences for the following article: '''\n",
        "prompt = ''\n",
        "\n",
        "for ex in examples:\n",
        "    prompt += task + ex + '\\n\\n'\n",
        "\n",
        "test = input()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "jb5dvsWRQ1kX",
        "outputId": "7c078530-63cd-40ea-9c35-fccd939b14dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.  Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt += task + test + '\\nSummary:'\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "811foelWR3zp",
        "outputId": "4c72bb46-340e-47ec-d34c-3ab28ae175cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provide a summary with about two sentences for the following article: Beyond our own products, we think it's important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we'll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\n",
            "Summary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\n",
            "\n",
            "Provide a summary with about two sentences for the following article: The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\n",
            "Summary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\n",
            "\n",
            "Provide a summary with about two sentences for the following article: We're also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We've already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we're sharing that we're now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We've used an AI technique called transfer learning to make it work in areas where there's less data available. We're also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We'll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\n",
            "Summary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there's less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\n",
            "\n",
            "Provide a summary with about two sentences for the following article: Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.  Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
            "Summary:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_large_language_model_sample(project_id=\"model-palace-388214\",\n",
        "                                    model_name=\"text-bison@001\",\n",
        "                                    temperature=0.8,\n",
        "                                    max_decode_steps=256,\n",
        "                                    top_p=0.95,\n",
        "                                    top_k=40,\n",
        "                                    content=prompt,\n",
        "                                    location=\"us-central1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "2kLXC5YQSTWI",
        "outputId": "86d6a9d5-11c4-46f8-ff20-5f3db505bbe3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:\n",
            "Natural language processing (NLP) is an interdisciplinary field that focuses on the interactions between computers and human language. The goal is to create computers that can \"understand\" the contents of documents, including the contextual nuances of the language within them. This technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: use the input function to have the model output a bio for your company's CEO\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "2ne6gD19u1ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a chatbot with the ChatModel\n",
        "\n",
        "It's now easy to imagine how one could build a toy chatbot. By wrapping the above code into a function and calling it until some condition is met (e.g. the user input == 'quit()', we could continuously generate model outputs for new test cases.\n",
        "\n",
        "However, Google also provides a chatbot model class called 'ChatModel', which works using a 'chat' object. Let's take a look.\n",
        "\n",
        "In the following function, we initialise a ChatModel and call its start_chat() function to create a chat object. We can then use this chat object outside of the function, in order to converse with the model:"
      ],
      "metadata": {
        "id": "znUYZlmVScAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "def start_llm_chat(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_output_tokens: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    location: str = \"us-central1\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "\n",
        "    chat_model = ChatModel.from_pretrained(model_name)\n",
        "    parameters = {\n",
        "      \"temperature\": temperature,\n",
        "      \"max_output_tokens\": max_output_tokens,\n",
        "      \"top_p\": top_p,\n",
        "      \"top_k\": top_k,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "        examples=[]\n",
        "    )\n",
        "\n",
        "    return chat "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xe8iKM7nePt-",
        "outputId": "94bb0822-d9d1-4ec2-87f7-3bef710b880f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'project_id': \"model-palace-388214\",\n",
        "    'model_name': \"chat-bison@001\",\n",
        "    'temperature': 0.2,\n",
        "    'max_output_tokens': 1024,\n",
        "    'top_p': 0.8,\n",
        "    'top_k': 40,\n",
        "    'location': \"us-central1\"\n",
        "}\n",
        "\n",
        "chat = start_llm_chat(**params)\n",
        "inp = \"\"\n",
        "while inp != \"quit\":\n",
        "    inp = input()\n",
        "    response=chat.send_message(inp)\n",
        "    print(f\"ChatBot: {response.text}\")\n",
        "    start_llm_chat(**params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "X7yjMWbgiF7D",
        "outputId": "72c11a92-decd-43d6-fbfd-9e535d05c9b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You said you're never gonna give me up. What else are you never gonna do?\n",
            "ChatBot: I'm never gonna give you up, I'm never gonna let you down, I'm never gonna run around and desert you. I'm never gonna make you cry, I'm never gonna say goodbye, I'm never gonna tell a lie and hurt you.\n",
            "\n",
            "I'm also never gonna judge you, I'm never gonna betray your trust, I'm never gonna take advantage of you, and I'm never gonna hurt you intentionally. I'm always gonna be there for you, no matter what. I'm your friend, and I'm always gonna be there for you\n",
            "Thankyou :)\n",
            "ChatBot: You're welcome! I'm glad I could help. I'm always here for you if you need anything.\n",
            "quit\n",
            "ChatBot: Okay, I'm quitting. I'm not sure what else to say. I've been working hard all day, and I'm just tired. I need a break. I'll be back later, when I'm feeling refreshed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bnfi8IfYfqf_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}